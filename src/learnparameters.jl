"""
	fit(options,trials)

RETURN a model with optimized parameters

ARGUMENT
-`options`: struct containing fixed hyperparameters
-`trials`: vector containing information about each trial
"""
function fit(options::Options, trials::Vector{<:Trial})
	model = Model(options,trials)
	fit!(model)
	return model
end

"""
	fit!(model)

Optimize model parameters

MODIFIED ARGUMENT
-`model`: a struct containing the data, hyperparameters, and parameters.
"""
function fit!(model::Model)
	if model.options.opt_method == "evidenceoptimization"
		maximizeevidence!(model)
	elseif model.options.opt_method == "gridsearch"
		gridsearch!(model)
	elseif model.options.opt_method == "maximumaposteriori"
		maximizeposterior!(model)
	end
	return nothing
end

"""
	maximizeposterior!(memory, model)

MODIFIED ARGUMENT
-`memory`: a struct pointing to pre-allocated memory for model optimization
-`model`: a struct containing the data, hyperparameters, and parameters. Only the parameters stored in the vector `ð°` are modified

RETURN
-a struct containing the results of the optimization (an instance of the composite type `Optim.MultivariateOptimizationResults`)
"""
function maximizeposterior!(memory::MemoryForOptimization, model::Model)
	memory.â„“[1] = NaN
	f(x) = neglogposterior!(memory,model,x)
	âˆ‡f!(âˆ‡,x) = âˆ‡neglogposterior!(âˆ‡,memory,model,x)
	âˆ‡âˆ‡f!(âˆ‡âˆ‡,x) = âˆ‡âˆ‡neglogposterior!(âˆ‡âˆ‡,memory,model,x)
    results = Optim.optimize(f, âˆ‡f!, âˆ‡âˆ‡f!, copy(model.ð°), NewtonTrustRegion(), Optim.Options(show_trace=true, iterations=model.options.opt_iterations_parameters))
	model.ð° .= Optim.minimizer(results)
	return results
end
maximizeposterior!(model::Model) = maximizeposterior!(MemoryForOptimization(model),model)

"""
	MemoryForOptimization(model)

RETURN a struct pointing to pre-allocated memory for model optimization
"""
function MemoryForOptimization(model::Model)
	N = length(model.ð°)
	MemoryForOptimization(â„“ = fill(NaN,1),
						  âˆ‡â„“ = fill(NaN, N),
						  âˆ‡âˆ‡â„“ = fill(NaN, N, N))
end

"""
	âˆ‡âˆ‡neglogposterior!(âˆ‡âˆ‡,model,â„“,âˆ‡â„“,âˆ‡âˆ‡â„“,x)

Hessian of the negative of the log-posterior

MODIFIED ARGUMENT
-`âˆ‡âˆ‡`: hessian matrix
-`model`: the field `ð°` is modified
-`â„“`: log-posterior
-`âˆ‡â„“`: gradient of the log-posterior
-`âˆ‡âˆ‡â„“`: hessian of the log-posterior

UNMODIFIED ARGUMENT
-`x`: parameter values
"""
function âˆ‡âˆ‡neglogposterior!(âˆ‡âˆ‡::Matrix{<:Real}, memory::MemoryForOptimization, model::Model, x::Vector{<:Real})
	âˆ‡âˆ‡logposterior!(memory,model,x)
	for i in eachindex(âˆ‡âˆ‡)
		âˆ‡âˆ‡[i] = -memory.âˆ‡âˆ‡â„“[i]
	end
	return nothing
end

"""
Gradient of the negative of the log-posterior
"""
function âˆ‡neglogposterior!(âˆ‡::Vector{<:Real}, memory::MemoryForOptimization, model::Model, x::Vector{<:Real})
	âˆ‡âˆ‡logposterior!(memory,model,x)
	for i in eachindex(âˆ‡)
		âˆ‡[i] = -memory.âˆ‡â„“[i]
	end
	return nothing
end

"""
Negative of the log-posterior
"""
function neglogposterior!(memory::MemoryForOptimization, model::Model, x::Vector{<:Real})
	âˆ‡âˆ‡logposterior!(memory,model,x)
	-memory.â„“[1]
end

"""
Hessian of the log-posterior
"""
function âˆ‡âˆ‡logposterior!(memory::MemoryForOptimization, model::Model, x::Vector{<:Real})
	if (x != model.ð°) || isnan(memory.â„“[1])
		for i in eachindex(x)
			model.ð°[i] = x[i]
		end
		âˆ‡âˆ‡logposterior!(memory,model)
	end
	return nothing
end

"""
	âˆ‡âˆ‡logposterior!(memory, model)

Log-posterior and its gradient and hessian

MODIFIED ARGUMENT
-`memory`: a struct pointing to pre-allocated memory for model optimization

UNMODIFIED ARGUMENT
-`model`: a struct containing the data, parameters, and hyperparamters
"""
function âˆ‡âˆ‡logposterior!(memory::MemoryForOptimization, model::Model)
	âˆ‡âˆ‡loglikelihood!(memory, model)
	@unpack â„“, âˆ‡â„“, âˆ‡âˆ‡â„“ = memory
    @unpack ð° = model
	Î± = model.a[1]
	â„“[1] -= 0.5Î±*(ð°â‹…ð°)
	for i in eachindex(âˆ‡â„“)
		âˆ‡â„“[i] -= Î±*ð°[i]
	    âˆ‡âˆ‡â„“[i,i] -= Î±
	end
	return nothing
end

"""
Log-likelihood and its gradient and hessian
"""
function âˆ‡âˆ‡loglikelihood!(memory::MemoryForOptimization, model::Model)
	@unpack â„“, âˆ‡â„“, âˆ‡âˆ‡â„“ = memory
    @unpack ð—, ð°, ð² = model
	Î”t = model.options.dt
	ð‹ = ð—*ð°
    â„“[1] = 0.0
    dÂ²ð¥_dð‹Â², dð¥_dð‹ = similar(ð‹), similar(ð‹)
    for t in eachindex(ð‹)
        dÂ²ð¥_dð‹Â²[t], dð¥_dð‹[t], â„“â‚œ = differentiate_twice_loglikelihood_wrt_linearpredictor(Î”t, ð‹[t], ð²[t])
	    â„“[1] += â„“â‚œ
    end
    âˆ‡â„“ .= ð—'*dð¥_dð‹
    âˆ‡âˆ‡â„“ .= ð—'*(dÂ²ð¥_dð‹Â².*ð—)
	return nothing
end

"""
Function used for testing with machine differentiation
"""
logposterior(a::Real, model::Model, ð°::Vector{<:Real}) = loglikelihood(model, ð°) - 0.5a*(ð°â‹…ð°)
logposterior(model::Model, ð°::Vector{<:Real}) = logposterior(model.a[1], model, ð°)

"""
Function used for testing with machine differentiation
"""
function loglikelihood(model::Model, ð°::Vector{<:Real})
	@unpack ð—, ð² = model
	Î”t = model.options.dt
	ð‹ = ð—*ð°
	â„“ = 0
	for (L,y) in zip(ð‹,ð²)
		â„“ += poissonloglikelihood(Î”t, L, y)
	end
	return â„“
end

"""
	gridsearch!(model)

Use cross-validation to search for the precision hyperparameter over a grid

MODIFIED ARGUMENT
-`model`: structure containing the parameters, hyperparameters, and data. The parameters and hyperparameters are updated.
"""
function gridsearch!(model::Model; ðš::Vector{<:Real}=10.0.^collect(-5:2), kfold::Integer=5)
	testindices, trainindices = SPGLM.cvpartition(kfold, length(model.trials))
	ð¥ = collect(loglikelihood(a, model, testindices, trainindices) for a in ðš)
	index = findmax(ð¥)[2]
	model.a[1] = ðš[index]
	model.ð° .= maximizeposterior(model,ðš[index])
end

"""
	loglikelihood(a,model,testindices,trainingindices)

RETURN the out-of-sample log-likelihood per time step

ARGUMENT
-`a`: precision of the Gaussian prior on the parameters
-`model`: structure containing the parameters, hyperparameters, and data. The parameters and hyperparameters are updated.
-`testindices`: nested vector whose each k-th element is a vector of integers indicating the indices of the samples used for testing in the k-th cross-validation fold
-`trainindices`: nested vector whose each k-th element is a vector of integers indicating the indices of the samples used for trainining in the k-th cross-validation
"""
function loglikelihood(a::AbstractFloat, model::Model, testindices::Vector{<:Vector{<:Integer}}, trainindices::Vector{<:Vector{<:Integer}})
	â„“ = 0
	T = length(model.ð²)
    for (testindices, trainindices) in zip(testindices, trainindices)
		trainingmodel = Model(model.options, model.trials[trainindices])
		testmodel = Model(model.options, model.trials[testindices])
		ð° = maximizeposterior(trainingmodel,a)
		testmodel.ð° .= ð°
		testmodel.a .= a
		trainingmodel.ð° .= ð°
		trainingmodel.a .= a
		LL = loglikelihood_each_timestep(testmodel,trainingmodel)
        â„“ += sum(sum.(LL))*sum(length.(LL))
    end
    â„“/T^2
end

"""
"""
function maximizeposterior(model::Model, a::Real)
	f(x) = -logposterior(a, model, x)
	results = optimize(f, model.ð°, LBFGS(); autodiff = :forward)
	Optim.minimizer(results)
end
