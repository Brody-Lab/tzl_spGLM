"""
	maximizeposterior!(memory, model)

MODIFIED ARGUMENT
-`memory`: a struct pointing to pre-allocated memory for model optimization
-`model`: a struct containing the data, hyperparameters, and parameters. Only the parameters stored in the vector `ğ°` are modified

RETURN
-a struct containing the results of the optimization (an instance of the composite type `Optim.MultivariateOptimizationResults`)
"""
function maximizeposterior!(memory::MemoryForOptimization, model::Model)
	memory.â„“[1] = NaN
	f(x) = neglogposterior!(memory,model,x)
	âˆ‡f!(âˆ‡,x) = âˆ‡neglogposterior!(âˆ‡,memory,model,x)
	âˆ‡âˆ‡f!(âˆ‡âˆ‡,x) = âˆ‡âˆ‡neglogposterior!(âˆ‡âˆ‡,memory,model,x)
    results = Optim.optimize(f, âˆ‡f!, âˆ‡âˆ‡f!, copy(model.ğ°), NewtonTrustRegion(), Optim.Options(show_trace=true, iterations=model.options.opt_iterations_parameters))
	model.ğ° .= Optim.minimizer(results)
	return results
end
maximizeposterior!(model::Model) = maximizeposterior!(MemoryForOptimization(model),model)

"""
	MemoryForOptimization(model)

RETURN a struct pointing to pre-allocated memory for model optimization
"""
function MemoryForOptimization(model::Model)
	N = length(model.ğ°)
	MemoryForOptimization(â„“ = fill(NaN,1),
						  âˆ‡â„“ = fill(NaN, N),
						  âˆ‡âˆ‡â„“ = fill(NaN, N, N))
end

"""
	âˆ‡âˆ‡neglogposterior!(âˆ‡âˆ‡,model,â„“,âˆ‡â„“,âˆ‡âˆ‡â„“,x)

Hessian of the negative of the log-posterior

MODIFIED ARGUMENT
-`âˆ‡âˆ‡`: hessian matrix
-`model`: the field `ğ°` is modified
-`â„“`: log-posterior
-`âˆ‡â„“`: gradient of the log-posterior
-`âˆ‡âˆ‡â„“`: hessian of the log-posterior

UNMODIFIED ARGUMENT
-`x`: parameter values
"""
function âˆ‡âˆ‡neglogposterior!(âˆ‡âˆ‡::Matrix{<:Real}, memory::MemoryForOptimization, model::Model, x::Vector{<:Real})
	âˆ‡âˆ‡logposterior!(memory,model,x)
	for i in eachindex(âˆ‡âˆ‡)
		âˆ‡âˆ‡[i] = -memory.âˆ‡âˆ‡â„“[i]
	end
	return nothing
end

"""
Gradient of the negative of the log-posterior
"""
function âˆ‡neglogposterior!(âˆ‡::Vector{<:Real}, memory::MemoryForOptimization, model::Model, x::Vector{<:Real})
	âˆ‡âˆ‡logposterior!(memory,model,x)
	for i in eachindex(âˆ‡)
		âˆ‡[i] = -memory.âˆ‡â„“[i]
	end
	return nothing
end

"""
Negative of the log-posterior
"""
function neglogposterior!(memory::MemoryForOptimization, model::Model, x::Vector{<:Real})
	âˆ‡âˆ‡logposterior!(memory,model,x)
	-memory.â„“[1]
end

"""
Hessian of the log-posterior
"""
function âˆ‡âˆ‡logposterior!(memory::MemoryForOptimization, model::Model, x::Vector{<:Real})
	if (x != model.ğ°) || isnan(memory.â„“[1])
		for i in eachindex(x)
			model.ğ°[i] = x[i]
		end
		âˆ‡âˆ‡logposterior!(memory,model)
	end
	return nothing
end

"""
	âˆ‡âˆ‡logposterior!(memory, model)

Log-posterior and its gradient and hessian

MODIFIED ARGUMENT
-`memory`: a struct pointing to pre-allocated memory for model optimization

UNMODIFIED ARGUMENT
-`model`: a struct containing the data, parameters, and hyperparamters
"""
function âˆ‡âˆ‡logposterior!(memory::MemoryForOptimization, model::Model)
	âˆ‡âˆ‡loglikelihood!(memory, model)
	@unpack â„“, âˆ‡â„“, âˆ‡âˆ‡â„“ = memory
    @unpack ğ° = model
	Î± = model.a[1]
	â„“[1] -= 0.5Î±*(ğ°â‹…ğ°)
	for i in eachindex(âˆ‡â„“)
		âˆ‡â„“[i] -= Î±*ğ°[i]
	    âˆ‡âˆ‡â„“[i,i] -= Î±
	end
	return nothing
end

"""
Log-likelihood and its gradient and hessian
"""
function âˆ‡âˆ‡loglikelihood!(memory::MemoryForOptimization, model::Model)
	@unpack â„“, âˆ‡â„“, âˆ‡âˆ‡â„“ = memory
    @unpack ğ—, ğ°, ğ² = model
	Î”t = model.options.dt
	ğ‹ = ğ—*ğ°
    â„“[1] = 0.0
    dÂ²ğ¥_dğ‹Â², dğ¥_dğ‹ = similar(ğ‹), similar(ğ‹)
    for t in eachindex(ğ‹)
        dÂ²ğ¥_dğ‹Â²[t], dğ¥_dğ‹[t], â„“â‚œ = differentiate_twice_loglikelihood_wrt_linearpredictor(Î”t, ğ‹[t], ğ²[t])
	    â„“[1] += â„“â‚œ
    end
    âˆ‡â„“ .= ğ—'*dğ¥_dğ‹
    âˆ‡âˆ‡â„“ .= ğ—'*(dÂ²ğ¥_dğ‹Â².*ğ—)
	return nothing
end

"""
Function used for testing with machine differentiation
"""
logposterior(a::Real, model::Model, ğ°::Vector{<:Real}) = loglikelihood(model, ğ°) - 0.5a*(ğ°â‹…ğ°)
logposterior(model::Model, ğ°::Vector{<:Real}) = logposterior(model.a[1], model, ğ°)

"""
Function used for testing with machine differentiation
"""
function loglikelihood(model::Model, ğ°::Vector{<:Real})
	@unpack ğ—, ğ² = model
	Î”t = model.options.dt
	ğ‹ = ğ—*ğ°
	â„“ = 0
	for (L,y) in zip(ğ‹,ğ²)
		â„“ += poissonloglikelihood(Î”t, L, y)
	end
	return â„“
end

"""
	gridsearch!(model)

Use cross-validation to search for the precision hyperparameter over a grid

MODIFIED ARGUMENT
-`model`: structure containing the parameters, hyperparameters, and data. The parameters and hyperparameters are updated.
"""
function gridsearch!(model::Model; ğš::Vector{<:Real}=10.0.^collect(-5:2), kfold::Integer=5)
	testindices, trainindices = SPGLM.cvpartition(kfold, length(model.trials))
	ğ¥ = collect(loglikelihood(a, model, testindices, trainindices) for a in ğš)
	index = findmax(ğ¥)[2]
	model.a[1] = ğš[index]
	model.ğ° .= maximizeposterior(model,ğš[index])
end

"""
	loglikelihood(a,model,testindices,trainingindices)

RETURN the out-of-sample log-likelihood per time step

ARGUMENT
-`a`: precision of the Gaussian prior on the parameters
-`model`: structure containing the parameters, hyperparameters, and data. The parameters and hyperparameters are updated.
-`testindices`: nested vector whose each k-th element is a vector of integers indicating the indices of the samples used for testing in the k-th cross-validation fold
-`trainindices`: nested vector whose each k-th element is a vector of integers indicating the indices of the samples used for trainining in the k-th cross-validation
"""
function loglikelihood(a::AbstractFloat, model::Model, testindices::Vector{<:Vector{<:Integer}}, trainindices::Vector{<:Vector{<:Integer}})
	â„“ = 0
	T = length(model.ğ²)
    for (testindices, trainindices) in zip(testindices, trainindices)
		trainingmodel = Model(model.options, model.trials[trainindices])
		testmodel = Model(model.options, model.trials[testindices])
		ğ° = maximizeposterior(trainingmodel,a)
		testmodel.ğ° .= ğ°
		testmodel.a .= a
		trainingmodel.ğ° .= ğ°
		trainingmodel.a .= a
		LL = loglikelihood_each_timestep(testmodel,trainingmodel)
        â„“ += sum(sum.(LL))*sum(length.(LL))
    end
    â„“/T^2
end

"""
"""
function maximizeposterior(model::Model, a::Real)
	f(x) = -logposterior(a, model, x)
	results = optimize(f, model.ğ°, LBFGS(); autodiff = :forward)
	Optim.minimizer(results)
end
