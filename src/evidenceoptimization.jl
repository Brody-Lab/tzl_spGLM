"""
	maximizeevidence!(model)

Learn both the parameters and hyperparameters by maximizing the approximate evidence

This optimization procedure alternately fixes the hyperparameters and learn the parameters by maximizing the posterior, and fixes the parameters and learn the hyperparameters by maximizing the evidence.

MODIFIED ARGUMENT
-`model`: structure containing the parameters, hyperparameters, and data. The parameters and hyperparameters are updated.

OPTIONAL ARGUMENT
-`verbose`: whether to display messages
"""
function maximizeevidence!(model::Model; verbose::Bool=true, MAP_convergence_g_tol::Real = 1e-5)
	bestğ¸ = -Inf
	bestğ° = copy(model.ğ°)
	bestğ‘ = model.a[1]
	memory = MemoryForOptimization(model)
	for i = 1:model.options.opt_iterations_hyperparameters
		verbose && println("Evidence optimization iteration: ", i, ": maximizing the log-posterior.")
	    Optim_results = maximizeposterior!(memory, model)
		MAP_converged = getfield(Optim_results, :g_residual) < MAP_convergence_g_tol
		if MAP_converged
			verbose && println("Evidence optimization iteration: ", i, ": MAP optimization converged")
		else
			verbose && println("Evidence optimization iteration: ", i, ": MAP optimization did not converge, and therefore the optimization procedure is aborting.")
			break
		end
		âˆ‡âˆ‡loglikelihood!(memory, model)
		ğ¸ = logevidence(memory, model)
		if ğ¸ > bestğ¸
			verbose && println("Evidence optimization iteration: ", i, ": the current log-evidence ( ", ğ¸, ") is greater than its previous value (", bestğ¸, ").")
			bestğ¸ = ğ¸
			bestğ° .= model.ğ°
			bestğ‘ = model.a[1]
		else
			verbose && println("Evidence optimization iteration: ", i, ": the current log-evidence ( ", ğ¸, ") is not greater than its previous value (", bestğ¸, "), and therefore the optimization procedure is aborting.")
			break
		end
		if i==model.options.opt_iterations_hyperparameters
			verbose && println("Evidence optimization iteration ", i, ": the last iteration has been reached, and the optimization procedure is aborting.")
			break
		end
		anew = maximizeevidence(memory, model)
		verbose && println("Evidence optimization iteration ", i, ": precision (a) ", model.a[1], " â†’ ", anew)
		model.a[1] = anew
	end
	if bestğ¸ > -Inf
		model.ğ° .= bestğ°
		model.a[1] = bestğ‘[1]
	end
	return nothing
end

"""
	maximizeevidence(memory, model)

RETURN precision parameter that maximizes the approximate evidence

ARGUMENT
-`memory`: structure containing variables to be modified during computations
-`model`: structure containing the parameters, hyperparameters, and data.
"""
function maximizeevidence(memory::MemoryForOptimization, model::Model)
	f(x) = -logevidence(memory, model, x[1])
	results = optimize(f, exp.(model.a), LBFGS(); autodiff = :forward)
	exp(Optim.minimizer(results)[1])
end

"""
	logevidence(memory, model, x)

RETURNS the approximate log evidence

ARGUMENT
-`a`: precision parameter
"""
function logevidence(a::Real, memory::MemoryForOptimization, model::Model)
	ğ‡ = memory.âˆ‡âˆ‡â„“
	ğ°â‚˜â‚â‚š = model.ğ°
	aâ‚€ = model.a[1]
	ğ° = (a*I - ğ‡) \ ((aâ‚€*I - ğ‡)*ğ°â‚˜â‚â‚š)
	loglikelihood(model,ğ°) - 0.5a*(ğ°â‹…ğ°) - 0.5logdet(I - ğ‡./a)
end
logevidence(memory::MemoryForOptimization, model::Model) = logevidence(model.a[1], memory, model)
logevidence(memory::MemoryForOptimization, model::Model, x::Real) = logevidence(exp(x), memory, model)
